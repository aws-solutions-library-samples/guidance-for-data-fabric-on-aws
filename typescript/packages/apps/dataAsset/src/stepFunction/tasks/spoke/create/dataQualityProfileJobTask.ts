import type { BaseLogger } from 'pino';
import type { DataAssetTask } from '../../models.js';
import { CreateDataQualityRulesetCommand, GlueClient, StartDataQualityRulesetEvaluationRunCommand, UpdateDataQualityRulesetCommand } from '@aws-sdk/client-glue';
import { ParameterType, PutParameterCommand, type SSMClient } from '@aws-sdk/client-ssm';
import { OpenLineageBuilder, RunEvent } from "@df/events";

export class DataQualityProfileJobTask {

    constructor(private readonly log: BaseLogger,
                private readonly glueClient: GlueClient,
                private readonly GlueDbName: string,
                private readonly ssmClient: SSMClient) {
    }

    public async process(event: DataAssetTask): Promise<any> {
        this.log.info(`DataQualityProfileJobTask > process > in > event: ${JSON.stringify(event)}`);

        const asset = event.dataAsset;
        const id = (asset.catalog?.assetId) ? asset.catalog.assetId : asset.requestId
        const jobName = `${asset.workflow.name}-${id}-dataQualityProfile`;

        const qualityRulesetCommandPayload = {
            Name: jobName,
            Ruleset: event.dataAsset.workflow.dataQuality.ruleset,
            Description: 'Data quality ruleset generated by Data Fabric.',
        }

        /**
         * Create the DataQualityRuleSet if it does not exist
         */
        try {
            await this.glueClient.send(new CreateDataQualityRulesetCommand({
                ...qualityRulesetCommandPayload,
                Tags: {
                    requestId: event.dataAsset.requestId,
                }
            }))
        } catch (e) {
            if (e.name === 'InvalidInputException' && e.message.includes('A resource with the same resourceName but a different internalId already exists')) {
                this.log.debug(`DataQualityProfileJobTask > process > ${e.message}`);
                await this.glueClient.send(new UpdateDataQualityRulesetCommand(qualityRulesetCommandPayload))
            } else {
                this.log.error(`DataQualityProfileJobTask > process > error: ${JSON.stringify(e)}`);
                throw e;
            }
        }

        /**
         * Run the DataQualityRulesetEvaluation
         */
        const startDataQualityRulesetEvaluationResponse = await this.glueClient.send(new StartDataQualityRulesetEvaluationRunCommand({
            RulesetNames: [jobName],
            DataSource: {
                GlueTable: {
                    TableName: event.dataAsset.execution.glueTableName,
                    DatabaseName: this.GlueDbName,
                }
            },
            Role: asset.workflow.roleArn
        }))

        /**
         * The lineage event for Data Quality will be augmented will the assertion results by DataQualityEventProcessor
         */
        const lineageEvent = this.constructDataLineage(event, startDataQualityRulesetEvaluationResponse.RunId);
        event.dataAsset.lineage.push(lineageEvent);

        // TODO: This will be replaced with utility function that will store it in S3.
        await this.ssmClient.send(new PutParameterCommand({
            Name: `/df/spoke/dataAsset/stateMachineExecution/create/${event.dataAsset.requestId}`,
            Value: JSON.stringify(event),
            Type: ParameterType.STRING,
            Overwrite: true
        }));

        this.log.info(`DataQualityProfileJobTask > process > exit:`);
    }

    private constructDataLineage(event: DataAssetTask, runId: string): Partial<RunEvent> {
        this.log.info(`DataQualityProfileJobTask > constructDataLineage > in> event: ${event}, runId: ${runId}`);

        const {catalog, workflow, execution} = event?.dataAsset;

        const openlineageBuilder = new OpenLineageBuilder();
        openlineageBuilder
            .setContext(catalog.domainId, catalog.domainName, execution.hubExecutionArn, [])
            .setJob(
                {
                    assetName: catalog.assetName,
                    jobName: `df_quality_profile_${runId}`
                })
            .setStartJob(
                {
                    executionId: runId,
                    startTime: new Date().toISOString(),
                    // TODO: where can I get the job name and runId of the parent
                    parent: {
                        name: workflow.name,
                        runId: execution.hubExecutionArn
                    }
                })
            .setDatasetInput(
                {
                    dataSource: {
                        url: 'TODO: who should provide the url for the data source'
                    },
                    name: workflow.dataset.name,
                    producer: 'TODO: what value should the producer be',
                    storage: {
                        storageLayer: workflow.dataset.format,
                        fileFormat: workflow.dataset.format
                    },
                    type: 'Custom',
                    version: "TODO is version optional?"
                });

        return openlineageBuilder.build();
    }
}
